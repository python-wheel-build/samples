diff --git a/vllm/lora/layers.py b/vllm/lora/layers.py
--- a/vllm/lora/layers.py
+++ b/vllm/lora/layers.py
@@ -439,8 +439,9 @@
                        if self.base_layer.skip_bias_add else None)
         return output, output_bias
 
+    # ReplicatedLinear should always be replaced, regardless of the fullyï¿¼
+    # sharded LoRAs setting, because it is, by definition, copied per GPU.
     @classmethod
-    @_not_fully_sharded_can_replace
     def can_replace_layer(
         cls,
         source_layer: nn.Module,
diff --git a/tests/lora/test_mixtral.py b/tests/lora/test_mixtral.py
--- a/tests/lora/test_mixtral.py
+++ b/tests/lora/test_mixtral.py
@@ -61,6 +61,7 @@
 
 
 @pytest.mark.parametrize("tp_size", [4])
+@pytest.mark.parametrize("fully_shard", [True, False])
 def test_mixtral_lora_all_target_modules(mixtral_lora_files_all_target_modules,
                                          tp_size):
     """This LoRA model has all supported Mixtral target modules"""
@@ -81,6 +82,7 @@
         max_loras=4,
         distributed_executor_backend="ray",
         tensor_parallel_size=tp_size,
+        fully_sharded_loras=fully_shard,
         max_lora_rank=32,
     )
 
